{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用前请确保 HM3D 的标签和配置对应的 `tar` 文件已经解压完成，但如果场景对应的 `tar` 文件过大，则无需解压，直接运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"width\": 256,\n",
    "    \"height\": 256,\n",
    "    'T_per_sample': 100, # 每个场景中，随机产生多少个位置\n",
    "    'rotation_step': 360.0, # 在每个位置上，产生该值除 360 个样本，相邻两个样本间摄像机朝向相差该值；该值为 360 表示每个位置采一个样本\n",
    "\n",
    "    # 数据集位置\n",
    "    \"dataset\": \"../../../data/hm3d_train/\",\n",
    "    \"dataset_config\": \"../../../data/hm3d_train/hm3d_annotated_train_basis.scene_dataset_config.json\", # Configuration of the dataset\n",
    "\n",
    "\n",
    "    # 采样结果的储存位置\n",
    "    \"rgb_path\": \"../../../data/rgbx/train/rgb\",\n",
    "    \"depth_path\": \"../../../data/rgbx/train/depth\",\n",
    "    \"semantic_path\": \"../../../data/rgbx/train/label\",\n",
    "\n",
    "    \"sensor_height\": 1.5,  # 传感器的高度，单位：米\n",
    "    \"enable_physics\": False, \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配置完成后运行剩下的全部代码，即可在 `settings['rgb_path'/'depth_path'/'semantic_path']` 下找到采样后的 RGB/D/Semantic 图像。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主体部分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化和存储\n",
    "\n",
    "如果需要改变存储的格式，请随意修改 `save_sample()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# function to display the topdown map\n",
    "from PIL import Image\n",
    "\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def save_sample(rgb_obs: np.ndarray, depth_obs: np.ndarray, semantic_obs: np.ndarray, rgb_path: str, depth_path: str, semantic_path: str):\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "    rgb_img.save(rgb_path)\n",
    "\n",
    "    np.save(depth_path, depth_obs)\n",
    "    np.save(semantic_path, semantic_obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habitat-Sim 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habitat_sim\n",
    "import sys\n",
    "sys.path.append('../../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfg(scene_dir, scene_name):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id =  os.path.join(settings['dataset'], scene_dir, scene_name + '.basis.glb')\n",
    "    sim_cfg.enable_physics = settings['enable_physics']\n",
    "    sim_cfg.scene_dataset_config_file = settings['dataset_config']\n",
    "\n",
    "\n",
    "    color_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_spec.uuid = 'color_sensor'\n",
    "    color_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_spec.resolution = [settings['height'], settings['width']]\n",
    "    color_sensor_spec.position = [0.0, settings['sensor_height'], 0.0]\n",
    "    color_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = 'depth_sensor'\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.position = [0.0, settings['sensor_height'], 0.0]\n",
    "    depth_sensor_spec.resolution = [settings['height'], settings['width']]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    semantic_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    semantic_sensor_spec.uuid = 'semantic_sensor'\n",
    "    semantic_sensor_spec.sensor_type = habitat_sim.SensorType.SEMANTIC\n",
    "    semantic_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    semantic_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    semantic_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    sensor_specs = [color_sensor_spec, depth_sensor_spec, semantic_sensor_spec]\n",
    "\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=settings['rotation_step'])\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=settings['rotation_step'])\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在单独场景中采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.perception.utils import ClassReducer\n",
    "def sample(scene_name, sim, T: int, rgb_path: str, depth_path: str, semantic_path: str, display: bool=False):\n",
    "    import random\n",
    "    import os\n",
    "    reducer = ClassReducer.from_sim(sim)\n",
    "    paths = {'color': rgb_path, 'depth': depth_path, 'semantic': semantic_path}\n",
    "    for t in range(T):\n",
    "        pos = None\n",
    "        while pos is None or not sim.pathfinder.is_navigable(pos):\n",
    "            pos = sim.pathfinder.get_random_navigable_point()\n",
    "        rot = np.array(random.random() * 2 * np.pi)\n",
    "        orientation = np.quaternion(np.cos(rot / 2), 0, -1 * np.sin(rot / 2), 0)\n",
    "        # orientation = np.quaternion(1, 0, 0, 0)\n",
    "        state = habitat_sim.AgentState(position=pos, rotation=orientation, sensor_states=sim.get_agent(0).get_state().sensor_states)\n",
    "        sim.get_agent(0).set_state(state=state, reset_sensors=False)\n",
    "\n",
    "        for k in range(int(360 / settings['rotation_step'])):\n",
    "            obs = sim.step('turn_right')\n",
    "            if display:\n",
    "                display_sample(obs['color_sensor'], reducer.instance_to_category(obs['semantic_sensor']), obs['depth_sensor'])\n",
    "            prefix = scene_name + \"_\" + f\"{pos}\".replace(' ','').replace('[', '').replace(']', '') + \"_\" + f\"{k}\"\n",
    "            save_sample(\n",
    "                obs['color_sensor'], obs['depth_sensor'], reducer.instance_to_category(obs['semantic_sensor']), \n",
    "                os.path.join(rgb_path, prefix + \".png\"),\n",
    "                os.path.join(depth_path, prefix + \".npy\"),\n",
    "                os.path.join(semantic_path, prefix + \".npy\"),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def make_samples_for_scene(scene_dir:str, scene_name:str=None):\n",
    "    if scene_name is None:\n",
    "        scene_name = scene_dir[scene_dir.find('-') + 1:]\n",
    "    cfg = make_cfg(scene_dir, scene_name)\n",
    "    \n",
    "    try:\n",
    "        sim.close()\n",
    "    except NameError:\n",
    "        pass\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "    sample(scene_name, sim, settings['T_per_sample'], settings['rgb_path'], settings['depth_path'], settings['semantic_path'])\n",
    "    sim.close()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在所有场景中采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import utils\n",
    "def make_samples():\n",
    "    # 无论 main.tar 是否解压，semantic.tar 都会给出有语义标记的目录\n",
    "    dirs = []\n",
    "    for dir in os.listdir(settings['dataset']):\n",
    "        if not os.path.isdir(os.path.join(settings['dataset'], dir)):\n",
    "            continue\n",
    "\n",
    "        contents = os.listdir(os.path.join(settings['dataset'], dir))\n",
    "        found_semantic = False\n",
    "        for c in contents:\n",
    "            if 'semantic' in c:\n",
    "                found_semantic = True\n",
    "        if not found_semantic:\n",
    "            continue\n",
    "        dirs.append(dir)\n",
    "    for dir in tqdm(dirs):\n",
    "        print(dir)\n",
    "        with utils.fulfill(settings['dataset'], dir):\n",
    "            make_samples_for_scene(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(os.path.join(settings['dataset'], 'main.tar')) as tar:\n",
    "    print([file for file in tar.getnames() if '00567' in file])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab258c8127595e7bcc65638ec3223783ec8b6f3f7de902030e2c0718ac7ad4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
