{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"width\": 256,\n",
    "    \"height\": 256,\n",
    "    'rotation_step': 30, # 旋转的粒度\n",
    "    'scale': 0.05, # 每个像素的大小，单位：米\n",
    "    'initial_grid': 1.0, # 枚举扫描位置时使用的初始间隔\n",
    "    'ignore': 0.20,\n",
    "    'threshold': 0.3,\n",
    "\n",
    "    # 数据集位置\n",
    "    \"dataset\": \"../../../data/hm3d_minival/\",\n",
    "    \"dataset_config\": \"../../../data/hm3d_minival/hm3d_annotated_minival_basis.scene_dataset_config.json\", # Configuration of the dataset\n",
    "\n",
    "\n",
    "    # 采样结果的储存位置\n",
    "    \"groundtruth_path\": \"../../../data/topdown/minival/\",\n",
    "\n",
    "    \"sensor_height\": 1.5,  # 传感器的高度，单位：米\n",
    "    \"enable_physics\": False, \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行全部"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "from models.common import device\n",
    "from models.perception.mapping.mapping import local_semantic_map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# function to display the topdown map\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def display_sample(rgb_obs, semantic_obs=np.array([]), depth_obs=np.array([])):\n",
    "    from habitat_sim.utils.common import d3_40_colors_rgb\n",
    "\n",
    "    rgb_img = Image.fromarray(rgb_obs, mode=\"RGBA\")\n",
    "\n",
    "    arr = [rgb_img]\n",
    "    titles = [\"rgb\"]\n",
    "    if semantic_obs.size != 0:\n",
    "        semantic_img = Image.new(\"P\", (semantic_obs.shape[1], semantic_obs.shape[0]))\n",
    "        semantic_img.putpalette(d3_40_colors_rgb.flatten())\n",
    "        semantic_img.putdata((semantic_obs.flatten() % 40).astype(np.uint8))\n",
    "        semantic_img = semantic_img.convert(\"RGBA\")\n",
    "        arr.append(semantic_img)\n",
    "        titles.append(\"semantic\")\n",
    "\n",
    "    if depth_obs.size != 0:\n",
    "        depth_img = Image.fromarray((depth_obs / 10 * 255).astype(np.uint8), mode=\"L\")\n",
    "        arr.append(depth_img)\n",
    "        titles.append(\"depth\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, data in enumerate(arr):\n",
    "        ax = plt.subplot(1, 3, i + 1)\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(titles[i])\n",
    "        plt.imshow(data)\n",
    "    plt.show(block=False)\n",
    "\n",
    "def display_map(topdown_map, key_points=None):\n",
    "    # plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(topdown_map)\n",
    "    # plot points on map\n",
    "    if key_points is not None:\n",
    "        for point in key_points:\n",
    "            plt.plot(point[0], point[1], marker=\"o\", markersize=10, alpha=0.8)\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Habitat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import habitat_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cfg(scene_dir, scene_name):\n",
    "    sim_cfg = habitat_sim.SimulatorConfiguration()\n",
    "    sim_cfg.gpu_device_id = 0\n",
    "    sim_cfg.scene_id =  os.path.join(settings['dataset'], scene_dir, scene_name + '.basis.glb')\n",
    "    sim_cfg.enable_physics = settings['enable_physics']\n",
    "    sim_cfg.scene_dataset_config_file = settings['dataset_config']\n",
    "\n",
    "\n",
    "    color_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    color_sensor_spec.uuid = 'color_sensor'\n",
    "    color_sensor_spec.sensor_type = habitat_sim.SensorType.COLOR\n",
    "    color_sensor_spec.resolution = [settings['height'], settings['width']]\n",
    "    color_sensor_spec.position = [0.0, settings['sensor_height'], 0.0]\n",
    "    color_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    depth_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    depth_sensor_spec.uuid = 'depth_sensor'\n",
    "    depth_sensor_spec.sensor_type = habitat_sim.SensorType.DEPTH\n",
    "    depth_sensor_spec.position = [0.0, settings['sensor_height'], 0.0]\n",
    "    depth_sensor_spec.resolution = [settings['height'], settings['width']]\n",
    "    depth_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    semantic_sensor_spec = habitat_sim.CameraSensorSpec()\n",
    "    semantic_sensor_spec.uuid = 'semantic_sensor'\n",
    "    semantic_sensor_spec.sensor_type = habitat_sim.SensorType.SEMANTIC\n",
    "    semantic_sensor_spec.resolution = [settings[\"height\"], settings[\"width\"]]\n",
    "    semantic_sensor_spec.position = [0.0, settings[\"sensor_height\"], 0.0]\n",
    "    semantic_sensor_spec.sensor_subtype = habitat_sim.SensorSubType.PINHOLE\n",
    "\n",
    "    sensor_specs = [color_sensor_spec, depth_sensor_spec, semantic_sensor_spec]\n",
    "\n",
    "    agent_cfg = habitat_sim.agent.AgentConfiguration()\n",
    "    agent_cfg.sensor_specifications = sensor_specs\n",
    "    agent_cfg.action_space = {\n",
    "        \"move_forward\": habitat_sim.agent.ActionSpec(\n",
    "            \"move_forward\", habitat_sim.agent.ActuationSpec(amount=0.25)\n",
    "        ),\n",
    "        \"turn_left\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_left\", habitat_sim.agent.ActuationSpec(amount=settings['rotation_step'])\n",
    "        ),\n",
    "        \"turn_right\": habitat_sim.agent.ActionSpec(\n",
    "            \"turn_right\", habitat_sim.agent.ActuationSpec(amount=settings['rotation_step'])\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    return habitat_sim.Configuration(sim_cfg, [agent_cfg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.perception.utils import (\n",
    "    quaternion2radian, \n",
    "    depth_map_to_point_cloud, \n",
    "    rotate_point_cloud, \n",
    "    fall_on_map,\n",
    "    ClassReducer,\n",
    "    display_semantic_map\n",
    ")\n",
    "\n",
    "def look_around(relative_position: tuple, sim: habitat_sim.Simulator, global_map: torch.Tensor, reducer: ClassReducer, tmp:torch.Tensor=None):\n",
    "    instance_map = []\n",
    "    depth_map = []\n",
    "    degrees = []\n",
    "\n",
    "    for t in range(int(360 / settings['rotation_step'])):\n",
    "        obs = sim.step('turn_right')\n",
    "        instance_map.append(torch.tensor(obs['semantic_sensor'] * 1.0).to(device))\n",
    "        depth_map.append(torch.tensor(obs['depth_sensor'] * 1.0).to(device))\n",
    "        degrees.append(quaternion2radian(sim.get_agent(0).get_state().rotation))\n",
    "\n",
    "\n",
    "    depth_map = torch.stack(depth_map)\n",
    "    instance_map = torch.stack(instance_map)\n",
    "    semantic_map = reducer.instance_to_category(instance_map)\n",
    "    assert (semantic_map != reducer.recover(reducer.reduce(semantic_map))).sum() == 0\n",
    "    semantic_map = reducer.reduce(semantic_map)\n",
    "    degrees = torch.stack(degrees).to(device)\n",
    "\n",
    "    fall_on_map(\n",
    "        rotate_point_cloud(\n",
    "            depth_map_to_point_cloud(\n",
    "                depth_map,\n",
    "                torch.tensor(-1.0).acos() / 2\n",
    "            ),\n",
    "            degrees\n",
    "        ),\n",
    "        semantic_map,\n",
    "        global_map,\n",
    "        relative_position,\n",
    "        settings['scale'],\n",
    "        tmp=tmp\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import max_pool2d, avg_pool2d\n",
    "\n",
    "def explore(sim, reducer: ClassReducer, global_semantic_map:torch.Tensor=None, tmp:torch.Tensor=None):\n",
    "    with torch.no_grad():\n",
    "        height = sim.pathfinder.get_bounds()[0][1]\n",
    "        bounds = sim.pathfinder.get_bounds()[0]\n",
    "        bounds = torch.tensor([bounds[2],bounds[0]]).to(device).flatten()\n",
    "\n",
    "        nav_mesh = torch.tensor(sim.pathfinder.get_topdown_view(settings['scale'], height)).to(device)\n",
    "        for t in range(10):\n",
    "            nav_mesh = nav_mesh | torch.tensor(sim.pathfinder.get_topdown_view(settings['scale'], height + t * 0.1)).to(device)\n",
    "        for t in range(10):\n",
    "            nav_mesh = nav_mesh | torch.tensor(sim.pathfinder.get_topdown_view(settings['scale'], height - t * 0.1)).to(device)\n",
    "\n",
    "        if global_semantic_map is None:\n",
    "            global_semantic_map = torch.zeros([reducer.get_reduced_class_number()] + list(nav_mesh.shape)).to(device)\n",
    "        if tmp is None:\n",
    "            tmp = torch.zeros(global_semantic_map.shape).to(device)\n",
    "\n",
    "        grid = settings['initial_grid']\n",
    "        count = 0\n",
    "        while True:\n",
    "            grid_px = int(grid / settings['scale'])\n",
    "            if grid_px <= 1 or grid_px <= settings['ignore'] / settings['scale']:\n",
    "                break\n",
    "\n",
    "            ys, xs = torch.arange(0, nav_mesh.shape[0]).to(device), torch.arange(0, nav_mesh.shape[1]).to(device)\n",
    "            ys, xs = ys.unsqueeze(dim=1).expand(-1, nav_mesh.shape[1]), xs.unsqueeze(dim=0).expand(nav_mesh.shape[0], -1)\n",
    "            coords, ys, xs = torch.stack([ys, xs], dim=0).type(torch.float) + 0.5, None, None\n",
    "            # coords 目前是像素坐标\n",
    "            coords = coords * nav_mesh.unsqueeze(dim=0)\n",
    "            navigatable = (max_pool2d(nav_mesh.unsqueeze(dim=0).type(torch.float), kernel_size=grid_px) > 0).reshape(-1)\n",
    "            coords = avg_pool2d(coords, kernel_size=grid_px).reshape(2, -1)[:, navigatable] / avg_pool2d(nav_mesh.unsqueeze(dim=0).type(torch.float), kernel_size=grid_px).reshape(-1)[navigatable]\n",
    "            # coords 目前包含所有可导航的粗像素的可导航中心\n",
    "\n",
    "            coarse_semantic_map = (avg_pool2d(global_semantic_map.max(dim=0)[0].unsqueeze(dim=0).clamp(max=1), kernel_size=grid_px) > settings['threshold']).squeeze(dim=0)\n",
    "            coords = coords[:, ~coarse_semantic_map.reshape(-1)[navigatable]]\n",
    "            # coords 目前包含所有可导航且不满足要求的粗像素的可导航中心\n",
    "\n",
    "\n",
    "            coords = coords * settings['scale']\n",
    "            coords = coords.transpose(0, 1)\n",
    "            abs_coords = coords + bounds\n",
    "\n",
    "            if len(coords) == 0 or count >= 10:\n",
    "                print(f\"grid={grid}m done\")\n",
    "                grid /= 2\n",
    "                continue\n",
    "            count += 1\n",
    "\n",
    "            for i in tqdm(range(len(coords)), f'grid={grid:.3f}m'):\n",
    "                pos = sim.pathfinder.get_random_navigable_point_near(\n",
    "                    [abs_coords[i][1].item(), height, abs_coords[i][0].item()], \n",
    "                    radius=settings['scale'] * grid_px\n",
    "                )\n",
    "                if not sim.pathfinder.is_navigable(pos):\n",
    "                    continue\n",
    "\n",
    "                state = habitat_sim.AgentState(\n",
    "                    position=pos,\n",
    "                    rotation=sim.get_agent(0).get_state().rotation\n",
    "                )\n",
    "                sim.get_agent(0).set_state(state=state, reset_sensors=False)\n",
    "                sim.step('turn_left')\n",
    "\n",
    "                pos = sim.get_agent(0).get_state().position\n",
    "                pos = torch.tensor([pos[2], pos[0]]).to(device)\n",
    "                relative_pos = pos - bounds\n",
    "                look_around(\n",
    "                    relative_pos,\n",
    "                    sim,\n",
    "                    global_semantic_map,\n",
    "                    reducer,\n",
    "                    tmp=tmp\n",
    "                )\n",
    "        return global_semantic_map, nav_mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scene(scene_name: str, global_semantic_map: torch.Tensor, reducer: ClassReducer):\n",
    "    path = os.path.join(settings['groundtruth_path'], scene_name)\n",
    "    torch.save(global_semantic_map, path + '.gsm')\n",
    "    reducer.save(path + '.reducer')\n",
    "\n",
    "def explore_scene(scene_dir:str, scene_name:str=None):\n",
    "    if scene_name is None:\n",
    "        scene_name = scene_dir[scene_dir.find('-') + 1:]\n",
    "    cfg = make_cfg(scene_dir, scene_name)\n",
    "    sim = habitat_sim.Simulator(cfg)\n",
    "\n",
    "    reducer = ClassReducer.from_sim(sim) \n",
    "    global_semantic_map, nav_mesh = explore(sim, reducer)\n",
    "\n",
    "    display_semantic_map(global_semantic_map)\n",
    "    display_map(nav_mesh.cpu().numpy())\n",
    "    save_scene(scene_name, global_semantic_map, reducer)\n",
    "\n",
    "    sim.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import utils\n",
    "def make_samples():\n",
    "    # 无论 main.tar 是否解压，semantic.tar 都会给出有语义标记的目录\n",
    "    dirs = []\n",
    "    for dir in os.listdir(settings['dataset']):\n",
    "        if not os.path.isdir(os.path.join(settings['dataset'], dir)):\n",
    "            continue\n",
    "\n",
    "        contents = os.listdir(os.path.join(settings['dataset'], dir))\n",
    "        found_semantic = False\n",
    "        for c in contents:\n",
    "            if 'semantic' in c:\n",
    "                found_semantic = True\n",
    "        if not found_semantic:\n",
    "            continue\n",
    "        dirs.append(dir)\n",
    "    for dir in tqdm(dirs):\n",
    "        print(dir)\n",
    "        with utils.fulfill(settings['dataset'], dir):\n",
    "            explore_scene(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MAGNUM_LOG\"] = \"quiet\"\n",
    "os.environ[\"HABITAT_SIM_LOG\"] = \"quiet\"\n",
    "make_samples()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab258c8127595e7bcc65638ec3223783ec8b6f3f7de902030e2c0718ac7ad4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
